{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP9AlVVkqaLFP1iR6GD/0Jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanga-v2/Transformers/blob/main/alammar_ch_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --upgrade\n",
        "!pip install -U accelerate"
      ],
      "metadata": {
        "id": "5RO3BRVDQjHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YMdUKPUP7MB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "soPua4fwRSjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "NBFGzIMmSGld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline"
      ],
      "metadata": {
        "id": "uWfK1W_7SZQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers.pipeline encapsulates the model, tokenizer, text generation process into a single function"
      ],
      "metadata": {
        "id": "OhdJrGhVSbLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "PXHEwNE9TC4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mygenerator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    return_full_text = False,\n",
        "    max_new_tokens = 100)"
      ],
      "metadata": {
        "id": "XkF4pVZvTIHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\" : \"user\",\n",
        "        \"content\" : \"assume you are robot, tell me how would you breakdown the other robot?\"\n",
        "    }\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "fV8fo0JMTqYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "4Op7HzppT72e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "7X8sTmujUiab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(accelerate.__version__)"
      ],
      "metadata": {
        "id": "Qjk2wGPfVfP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = mygenerator(messages)"
      ],
      "metadata": {
        "id": "wv6PgNHUT-ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "K6ceuFCgXi-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]['generated_text']"
      ],
      "metadata": {
        "id": "A7wOcph9XmXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Tell me a joke about robots\"}]\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "output = pipe(messages[0][\"content\"], max_new_tokens=100)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "AKK062KgXDR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}